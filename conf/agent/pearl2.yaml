name: pearl2
tau: 0.005 # for SAC target network update
pi_lr: 3E-4
q_lr: 3E-4
vf_lr: 3E-4 # is not used, instead value network uses same lr as q network
context_lr: 3E-4
gamma: 0.99 # RL discount factor
kl_lambda: 0.1 # weight on KL divergence term in encoder loss
policy_mean_reg_weight: 1E-3
policy_std_reg_weight: 1E-3
policy_pre_activation_weight: 0
use_information_bottleneck: True # False makes latent context deterministic
use_next_obs_in_context: False # use next obs if it is useful in distinguishing tasks
latent_size: 5 # dimension of latent space in context encoder
net_size: 300 # number of neurons in hidden layer
reward_scale: 2.0 # scale rewards before constructing Bellman update, effectively controls weight on the entropy of the policy
context_size: 64