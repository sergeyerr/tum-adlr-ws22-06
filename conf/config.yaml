agent:
  name: pearl # this is only used to determine which of the sub groups to use

  pearl:
    experiment_name: pearl_default_env
    tau: 0.005 # for SAC target network update
    pi_lr: 3E-4
    q_lr: 3E-4
    vf_lr: 3E-4 # is not used, instead value network uses same lr as q network
    context_lr: 3E-4
    gamma: 0.99 # RL discount factor
    kl_lambda: 0.1 # weight on KL divergence term in encoder loss
    policy_mean_reg_weight: 1E-3
    policy_std_reg_weight: 1E-3
    policy_pre_activation_weight: 0
    use_information_bottleneck: True # False makes latent context deterministic
    use_next_obs_in_context: False # use next obs if it is useful in distinguishing tasks
    latent_size: 5 # dimension of latent space in context encoder
    net_size: 300 # number of neurons in hidden layer
    reward_scale: 2.0 # scale rewards before constructing Bellman update, effectively controls weight on the entropy of the policy

  sac:
    pi_lr: 0.0003
    q_lr: 0.0003
    gamma: 0.99
    tau: 0.005
    reward_scale: 2
    experiment_name: sac_default_env

  sac2:
    pi_lr: 0.0001
    q_lr: 0.001
    gamma: 0.99
    tau: 0.005
    reward_scale: 0.2
    alpha: 1
    experiment_name: sac2_default_env

training:
  n_train_tasks: 100
  pass_env_parameters: False
  random: True
  episodes: 500
  num_initial_steps: 1000 # number of transitions collected per task before training
  num_train_steps_per_itr: 100 # 2000
  num_tasks_sample: 5 # number of randomly sampled tasks to collect data for each iteration
  seed: 0
  noise: 'Ornstein' #'Zero','Gaussian', 'Ornstein'
  noise_param: 0.2
  max_path_length: 1000 #200 max path length for this environment


  pearl:
    meta_batch: 32 # number of tasks to average the gradient across 64
    num_steps_prior: 600 # number of transitions to collect per task with z ~ prior
    num_steps_posterior: 0 # number of transitions to collect per task with z ~ posterior
    num_extra_rl_steps_posterior: 600 # number of additional transitions to collect per task with z ~ posterior that are only used to train the policy and NOT the encoder
    # num_steps_per_eval: 600  # nuumber of transitions to eval on
    batch_size: 256 # number of transitions in the RL batch 1024
    embedding_batch_size: 64 # number of transitions in the context batch 1024
    update_post_train: 1 # how often to resample the context when collecting data during training (in trajectories)
    replay_buffer_size: 1000000

  sac:
    episode_length: 2200 # I make this approximately the sum of num_steps_prior, num_steps_posterior, num_extra_rl_steps_posterior, num_initial_steps
    batch_size: 256
    train_batches: 1
    update_after: 1000
    update_every: 1
    min_replay_size: 1000
    replay_buffer_size: 1000000

  sac2:
    episode_length: 2200 # I make this approximately the sum of num_steps_prior, num_steps_posterior, num_extra_rl_steps_posterior, num_initial_steps
    batch_size: 64
    train_batches: 1
    update_after: 1000
    update_every: 1
    min_replay_size: 1000
    replay_buffer_size: 1000000

validation:
  log_model_wandb: True
  log_model_every_training_batch: 1000

  # hypercube validation is not possible (it makes no sense), if env is NOT random 
  hypercube_validation: True
  hypercube_points_per_axis: 3 # maybe it makes sense to have 4 points on gravity axis, to not only test the extremes
  # eval_eps will be calculated automatically in case of hypercube validation
  eval_eps: 1 #50
  n_eval_tasks: 5 #in case of hypecube validation this will be overwritten by hypercube_points_per_axis

  eval_stop_condition: 'min' # 'min', 'avg': how results from eval_eps are aggregated
  eval_interval: 10
  record_video_on_eval: True
  validation_episode_length: 1000
  validation_traj_num: 2 # must be larger than num_exp_traj_eval, else pearl cannot collect context to inform the policy
  num_exp_traj_eval: 1 # must be at least 1 and smaller than validation_traj_num
  log_actions: True


env:
  seed: 0
  deterministic_reset: True
  random: True
  random_type: 'Uniform' # 'Uniform', 'Gaussian'
  # if you set any of bounds precisely, there will be the error in validation
  gravity_lower: -11.999 # -11.999 from -12 to 0
  gravity_upper: -1.0 # -0.0001 from -12 to 0
  wind_probability: 0.5
  wind_power_lower: 0 # from 0 to 20
  wind_power_upper: 10 # from 0 to 20
  turbulence_power_lower: 0 # from 0 to 2
  turbulence_power_upper: 1  # from 0 to 2

  # If randomize type == Fixed, then these parameters will be used always
  default_gravity: -10
  default_wind: False
  default_wind_power: 0
  default_turbulence_power: 1.5


sweep:
  activate: False

  
  

