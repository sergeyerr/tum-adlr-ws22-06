meta_batch: 32 # number of tasks to average the gradient across 64
num_steps_prior: 600 # number of transitions to collect per task with z ~ prior
num_steps_posterior: 0 # number of transitions to collect per task with z ~ posterior
num_extra_rl_steps_posterior: 600 # number of additional transitions to collect per task with z ~ posterior that are only used to train the policy and NOT the encoder
# num_steps_per_eval: 600  # nuumber of transitions to eval on
batch_size: 256 # number of transitions in the RL batch 1024
embedding_batch_size: 64 # number of transitions in the context batch 1024
update_post_train: 1 # how often to resample the context when collecting data during training (in trajectories)
replay_buffer_size: 1000000